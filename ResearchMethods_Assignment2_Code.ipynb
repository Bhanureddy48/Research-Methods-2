{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOK2NpKkzOF8mQyATvBnPwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhanureddy48/Research-Methods-2/blob/main/ResearchMethods_Assignment2_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data handling and visualization libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# String module for punctuation handling\n",
        "import string\n",
        "# Scikit-learn for splitting data into train and evaluation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load T5 tokenizer and model for English-to-French translation\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "# Hugging Face Datasets library for managing datasets\n",
        "from datasets import Dataset\n",
        "# Data collator for dynamic padding during training\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "# Install or upgrade Hugging Face Transformers library\n",
        "!pip install -U transformers --quiet\n",
        "!pip install --upgrade transformers --quiet\n",
        "!pip install evaluate --quiet\n",
        "# Install necessary metric dependencies for evaluate\n",
        "!pip install sacrebleu rouge-score nltk --quiet\n",
        "# Training configuration and training utilities from Transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "# PyTorch for model input and device management\n",
        "import torch\n",
        "# tqdm for progress bars during generation\n",
        "from tqdm import tqdm\n",
        "# Hugging Face Evaluate library for BLEU, ROUGE, METEOR, chrF++ metrics\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "SMOMa2zQ0W3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRG4_WIMSaSw"
      },
      "outputs": [],
      "source": [
        "# Define the URL to the English-French translation dataset\n",
        "eng_to_fr_csv_url = \"https://raw.githubusercontent.com/Bhanureddy48/Research-Methods-2/refs/heads/main/eng_-french.csv\"\n",
        "# Load only the first 20,000 rows from the dataset\n",
        "eng_french_df = pd.read_csv(eng_to_fr_csv_url, nrows=20000)\n",
        "# Rename columns for clarity\n",
        "eng_french_df.columns = [\"english_text\", \"french_text\"]\n",
        "# Display the first 5 English-French sentence pairs\n",
        "print(\"Selected Set of English-French sentence pairs :\")\n",
        "print(eng_french_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "eng_french_df.info()"
      ],
      "metadata": {
        "id": "ZnvWtEfCTHL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show basic statistics for the English and French text columns\n",
        "print(eng_french_df.describe(include=\"object\"))"
      ],
      "metadata": {
        "id": "0vTg4RDmTLJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing (null) values in each column\n",
        "print(eng_french_df.isnull().sum())"
      ],
      "metadata": {
        "id": "MvRnKrWFTTHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many duplicate sentence pairs exist in the dataset\n",
        "Eng_Fre_DupCnt = eng_french_df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {Eng_Fre_DupCnt}\")"
      ],
      "metadata": {
        "id": "ESkPTcqqTYHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add sentence length columns (number of words) to the DataFrame\n",
        "eng_french_df[\"english_length\"] = eng_french_df[\"english_text\"].apply(lambda x: len(x.split()))\n",
        "eng_french_df[\"french_length\"] = eng_french_df[\"french_text\"].apply(lambda x: len(x.split()))\n",
        "# Display basic statistics about sentence lengths\n",
        "print(\"Sentence length statistics (word count):\")\n",
        "print(eng_french_df[[\"english_length\", \"french_length\"]].describe())"
      ],
      "metadata": {
        "id": "xm6E_lLsThI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with 1 row and 2 columns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# English sentence length distribution with density line\n",
        "sns.histplot(eng_french_df[\"english_length\"], bins=20, kde=True, ax=axes[0], color=\"skyblue\", edgecolor=\"black\")\n",
        "axes[0].set_title(\"English Sentence Length (Word Count)\")\n",
        "axes[0].set_xlabel(\"Number of Words\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].grid(False)\n",
        "# French sentence length distribution with density line\n",
        "sns.histplot(eng_french_df[\"french_length\"], bins=20, kde=True, ax=axes[1], color=\"lightgreen\", edgecolor=\"black\")\n",
        "axes[1].set_title(\"French Sentence Length (Word Count)\")\n",
        "axes[1].set_xlabel(\"Number of Words\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].grid(False)\n",
        "# Improve spacing between plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2iRzAKq8Tp0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_translation_pairs_dataframe(translation_df, min_char_count=2, max_word_limit=30, strip_punctuation=False):\n",
        "    \"\"\"\n",
        "    This function cleans and preprocesses a DataFrame containing English-to-French\n",
        "    sentence pairs. It standardizes the text by converting all characters to\n",
        "    lowercase, trimming leading and trailing whitespace, and optionally removing\n",
        "    punctuation. Sentence pairs that are too short or too long based on character\n",
        "    count or word count thresholds are filtered out. The cleaned English and French\n",
        "    sentences, along with their respective word counts, are stored in new columns.\n",
        "    The final cleaned DataFrame is returned for use in modeling or further analysis.\n",
        "\n",
        "    Parameters:\n",
        "        translation_df (pd.DataFrame): The input DataFrame containing\n",
        "            'english_text' and 'french_text' columns.\n",
        "        min_char_count (int): Minimum number of characters required for both\n",
        "            English and French sentences.\n",
        "        max_word_limit (int): Maximum number of words allowed in each sentence.\n",
        "        strip_punctuation (bool): If True, removes punctuation from both English\n",
        "            and French text.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned DataFrame with the following columns:\n",
        "            - 'english_text', 'french_text' (original sentences),\n",
        "            - 'english_cleaned', 'french_cleaned' (preprocessed text),\n",
        "            - 'english_sentence_length', 'french_sentence_length' (word counts).\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid changing the original DataFrame\n",
        "    cleaned_translation_df = translation_df.copy()\n",
        "    # Convert sentences to lowercase and strip whitespace\n",
        "    cleaned_translation_df[\"english_cleaned\"] = cleaned_translation_df[\"english_text\"].str.lower().str.strip()\n",
        "    cleaned_translation_df[\"french_cleaned\"] = cleaned_translation_df[\"french_text\"].str.lower().str.strip()\n",
        "    # remove punctuation from sentences\n",
        "    if strip_punctuation:\n",
        "        remove_punct_func = lambda txt: txt.translate(str.maketrans('', '', string.punctuation))\n",
        "        cleaned_translation_df[\"english_cleaned\"] = cleaned_translation_df[\"english_cleaned\"].apply(remove_punct_func)\n",
        "        cleaned_translation_df[\"french_cleaned\"] = cleaned_translation_df[\"french_cleaned\"].apply(remove_punct_func)\n",
        "    # Filter out sentences that are shorter than the minimum character requirement\n",
        "    cleaned_translation_df = cleaned_translation_df[\n",
        "        (cleaned_translation_df[\"english_cleaned\"].str.len() >= min_char_count) &\n",
        "        (cleaned_translation_df[\"french_cleaned\"].str.len() >= min_char_count)].reset_index(drop=True)\n",
        "    # Calculate word counts for both English and French sentences\n",
        "    cleaned_translation_df[\"english_sentence_length\"] = cleaned_translation_df[\"english_cleaned\"].apply(lambda x: len(x.split()))\n",
        "    cleaned_translation_df[\"french_sentence_length\"] = cleaned_translation_df[\"french_cleaned\"].apply(lambda x: len(x.split()))\n",
        "    # Filter out sentences that exceed the maximum word count\n",
        "    cleaned_translation_df = cleaned_translation_df[\n",
        "        (cleaned_translation_df[\"english_sentence_length\"] <= max_word_limit) &\n",
        "        (cleaned_translation_df[\"french_sentence_length\"] <= max_word_limit)\n",
        "    ].reset_index(drop=True)\n",
        "    return cleaned_translation_df"
      ],
      "metadata": {
        "id": "2msUWO6EVfsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the translation cleaning function to the existing English-French DataFrame\n",
        "cleaned_translation_df = clean_translation_pairs_dataframe(eng_french_df, strip_punctuation=True)\n",
        "# Display original and cleaned sentence pairs\n",
        "print(cleaned_translation_df[[\"english_text\", \"english_cleaned\", \"french_text\", \"french_cleaned\"]].head())"
      ],
      "metadata": {
        "id": "kCJJV0YaXPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split cleaned English-French sentence pairs into training and evaluation sets (90% train, 10% eval)\n",
        "train_translation_df, eval_translation_df = train_test_split(\n",
        "    cleaned_translation_df[[\"english_cleaned\", \"french_cleaned\"]],\n",
        "    test_size=0.2,random_state=99999)\n",
        "# Reset index for both training and evaluation DataFrames\n",
        "train_translation_df = train_translation_df.reset_index(drop=True)\n",
        "eval_translation_df = eval_translation_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "vfuqDy2vWJn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained T5 tokenizer and model for English-to-French translation\n",
        "translation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "translation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "6mw6qzFpWO0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert training and evaluation DataFrames to Hugging Face Datasets\n",
        "train_hf_dataset = Dataset.from_pandas(train_translation_df)\n",
        "eval_hf_dataset = Dataset.from_pandas(eval_translation_df)\n",
        "def tokenize_translation_pair(example):\n",
        "    \"\"\"\n",
        "    Tokenizes a single English-to-French sentence pair for input into a T5 model.\n",
        "    It prepends the translation prefix required by the model to the English input,\n",
        "    tokenizes both English and French sentences, and aligns them as input and labels.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): A dictionary containing 'english_cleaned' and 'french_cleaned' keys.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with tokenized input IDs, attention masks, and labels for training.\n",
        "    \"\"\"\n",
        "    # Add the translation prefix expected by the T5 model\n",
        "    source_text = \"translate English to French: \" + example[\"english_cleaned\"]\n",
        "    target_text = example[\"french_cleaned\"]\n",
        "    # Tokenize the source (English) sentence\n",
        "    model_inputs = translation_tokenizer(\n",
        "        source_text,max_length=128,\n",
        "        padding=\"max_length\",truncation=True)\n",
        "    # Tokenize the target (French) sentence as the label\n",
        "    with translation_tokenizer.as_target_tokenizer():\n",
        "        label_tokens = translation_tokenizer(\n",
        "            target_text,max_length=128,\n",
        "            padding=\"max_length\",truncation=True\n",
        "        )\n",
        "    # Assign tokenized labels to the model inputs\n",
        "    model_inputs[\"labels\"] = label_tokens[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "3w1cVVcmWQmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training and evaluation datasets using the custom tokenization function\n",
        "train_tokenized_dataset = train_hf_dataset.map(tokenize_translation_pair, batched=False)\n",
        "eval_tokenized_dataset = eval_hf_dataset.map(tokenize_translation_pair, batched=False)"
      ],
      "metadata": {
        "id": "3nXb_oqbWTBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator to dynamically pad inputs and labels for the translation task\n",
        "translation_data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=translation_tokenizer,model=translation_model)"
      ],
      "metadata": {
        "id": "XWeS3H3NWZDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training configuration for English-to-French translation using T5\n",
        "translation_training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_translation_eng_to_fr_results\",\n",
        "    do_train=True,do_eval=True,per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,learning_rate=3e-4,\n",
        "    num_train_epochs=3,weight_decay=0.01,\n",
        "    logging_dir=\"./t5_translation_logs\", logging_steps=50,\n",
        "    save_total_limit=2, report_to=\"none\")"
      ],
      "metadata": {
        "id": "Ec1GtVSoXAeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer for English-to-French translation fine-tuning\n",
        "translation_trainer = Trainer(\n",
        "    model=translation_model,\n",
        "    args=translation_training_args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=eval_tokenized_dataset,\n",
        "    tokenizer=translation_tokenizer,         # Tokenizer used during training\n",
        "    data_collator=translation_data_collator  # Handles dynamic padding\n",
        ")"
      ],
      "metadata": {
        "id": "hvegVMVrWrN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training the translation model\n",
        "translation_train_output = translation_trainer.train()"
      ],
      "metadata": {
        "id": "HN6T0MKzWs9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 200 samples from the evaluation set\n",
        "translation_eval_df = eval_translation_df.sample(n=500, random_state=9999).reset_index(drop=True)\n",
        "# Create task-formatted input strings for translation\n",
        "eng_french_input_texts = [ \"translate English to French: \" + text\n",
        "    for text in translation_eval_df[\"english_cleaned\"].tolist()]\n",
        "# Extract the ground truth French references\n",
        "french_reference_texts = translation_eval_df[\"french_cleaned\"].tolist()\n",
        "# Tokenize the input prompts for model processing\n",
        "eng_french_tokenized_inputs = translation_tokenizer(\n",
        "    eng_french_input_texts,return_tensors=\"pt\",\n",
        "    padding=True,truncation=True,\n",
        "    max_length=128).to(translation_model.device)\n",
        "# Generate predictions from the translation model\n",
        "with torch.no_grad():\n",
        "    eng_french_generated_ids = translation_model.generate(\n",
        "        input_ids=eng_french_tokenized_inputs[\"input_ids\"],\n",
        "        attention_mask=eng_french_tokenized_inputs[\"attention_mask\"],\n",
        "        max_length=128,num_beams=4)\n",
        "# Decode generated token IDs into actual French sentences\n",
        "french_predicted_texts = translation_tokenizer.batch_decode(eng_french_generated_ids,\n",
        "    skip_special_tokens=True)\n",
        "# Load evaluation metrics\n",
        "eng_french_bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "eng_french_rouge_metric = evaluate.load(\"rouge\")\n",
        "eng_french_chrf_metric = evaluate.load(\"chrf\")\n",
        "eng_french_meteor_metric = evaluate.load(\"meteor\")\n",
        "# Compute scores by comparing predicted and reference translations\n",
        "eng_french_bleu_score = eng_french_bleu_metric.compute(predictions=french_predicted_texts,\n",
        "    references=[[ref] for ref in french_reference_texts])[\"score\"]\n",
        "eng_french_rouge_score = eng_french_rouge_metric.compute(predictions=french_predicted_texts,\n",
        "    references=french_reference_texts)[\"rougeL\"]\n",
        "eng_french_chrf_score = eng_french_chrf_metric.compute(predictions=french_predicted_texts,\n",
        "    references=french_reference_texts)[\"score\"]\n",
        "eng_french_meteor_score = eng_french_meteor_metric.compute(predictions=french_predicted_texts,\n",
        "                                  references=french_reference_texts)[\"meteor\"]\n",
        "# Pack evaluation results into a dictionary\n",
        "eng_french_translation_scores = {\n",
        "    \"BLEU\": eng_french_bleu_score,\n",
        "    \"ROUGE-L\": eng_french_rouge_score,\n",
        "    \"chrF++\": eng_french_chrf_score,\n",
        "    \"METEOR\": eng_french_meteor_score\n",
        "}\n",
        "# Print formatted evaluation results\n",
        "print(\"\\nEvaluation on Sample Examples:\")\n",
        "for metric_name, score in eng_french_translation_scores.items():\n",
        "    print(f\"{metric_name}: {score:.2f}\")"
      ],
      "metadata": {
        "id": "yltvcMsobT_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to compare input, reference, and predicted translations\n",
        "sample_translation_output_df = pd.DataFrame({\n",
        "    \"English_Input\": eng_french_input_texts[:50],\n",
        "    \"French_Reference\": french_reference_texts[:50],\n",
        "    \"French_Predicted\": french_predicted_texts[:50]\n",
        "})\n",
        "# Ensure full text is visible in each cell\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "# Display the top 20 comparison results\n",
        "sample_translation_output_df.head(50)"
      ],
      "metadata": {
        "id": "DYPOs99VcZPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}